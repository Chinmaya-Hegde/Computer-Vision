# -*- coding: utf-8 -*-
"""CVDL_R.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XCtKmXGloTNA2Yn22xbZq4Kyt0acvvIN
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import pathlib
import matplotlib.pyplot as plt
import numpy as np
from google.colab import drive
import os
from tensorflow.keras import datasets
from tensorflow.keras.layers import Input, Dense, Conv2D,Flatten,MaxPooling2D,Dropout
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.regularizers import l2
from tensorflow.keras.optimizers import SGD

drive.mount('/content/drive')
os.chdir("/content/drive/My Drive/Colab Notebooks")
data_dir=pathlib.Path('train')
valid_dir=pathlib.Path('valid')
test_dir=pathlib.Path('test')

image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=40,
                                                                  width_shift_range=0.2,
                                                                  height_shift_range=0.2,
                                                                  shear_range=0.2,
                                                                  zoom_range=0.2,
                                                                  fill_mode='nearest',
                                                                  channel_shift_range=10,
                                                                  horizontal_flip= True, 
                                                                  rescale=1./255)
image_generator2 = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)
image_generator3 = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)

BATCH_SIZE = 64
IMG_HEIGHT = 224
IMG_WIDTH = 224
# STEPS_PER_EPOCH = np.ceil(image_count/BATCH_SIZE)
CLASS_NAMES = np.array([item.name for item in data_dir.glob('*') if item.name != ".DS_Store"])

train_data_gen = image_generator.flow_from_directory(directory=str(data_dir),
                                                     batch_size=64,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                     classes = list(CLASS_NAMES), class_mode='categorical')
valid_data_gen = image_generator2.flow_from_directory(directory=str(valid_dir),
                                                     batch_size=64,
                                                     shuffle=True,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                     classes = list(CLASS_NAMES), class_mode='categorical')
test_data_gen = image_generator3.flow_from_directory(directory=str(test_dir),
                                                     batch_size=64,
                                                     shuffle=False,
                                                     target_size=(IMG_HEIGHT, IMG_WIDTH),
                                                     classes = list(CLASS_NAMES), class_mode='categorical')
train_data_gen
valid_data_gen

model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(0.0005), padding='same', input_shape=(224, 224, 3)))
model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0005), padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.2))
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(0.0005), padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',kernel_regularizer=l2(0.0005), padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.3))
model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0005), padding='same'))
model.add(BatchNormalization())
model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(0.0005), padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D((2, 2)))
model.add(Dropout(0.4))
model.add(Flatten())
model.add(Dense(256, activation='relu',kernel_regularizer=l2(0.0005), kernel_initializer='he_uniform') )
model.add(BatchNormalization())
model.add(Dropout(0.5))
model.add(Dense(102, activation='softmax'))
model.summary()

model.load_weights('Trained3.h5')

opt = tf.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

from tensorflow.keras.callbacks import ModelCheckpoint
filepath="weights.best.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')
callbacks_list = [checkpoint]

model.fit_generator(generator=train_data_gen,
                   validation_data= valid_data_gen,callbacks=callbacks_list,
    epochs=100
)

model.save('Trained3.h5')

import matplotlib.pyplot as plt
# print(model.history.history)
# Plot training & validation accuracy values
plt.plot(model.history.history['accuracy'])
plt.plot(model.history.history['val_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

import matplotlib.pyplot as plt
# print(model.history.history)
# Plot training & validation accuracy values
plt.plot(model.history.history['loss'])
plt.plot(model.history.history['val_loss'])
plt.title('Model accuracy')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

model.evaluate_generator(test_data_gen)

import numpy as np
test_steps_per_epoch = np.math.ceil(test_data_gen.samples / test_data_gen.batch_size)

predictions = model.predict_generator(test_data_gen, steps=test_steps_per_epoch)
# Get most likely class
predicted_classes = np.argmax(predictions, axis=1)

predicted_classes

true_classes = test_data_gen.classes
class_labels = list(test_data_gen.class_indices.keys())

from sklearn import metrics
report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)

metrics.accuracy_score(true_classes,predicted_classes)

print(*true_classes, sep = ", ")

print(predicted_classes)

print(*predicted_classes, sep = ", ")

confusion_matrix = metrics.precision_recall_fscore_support(y_true=true_classes, y_pred=predicted_classes)

confusion_matrix

tf.math.confusion_matrix(true_classes, predicted_classes)

import pandas as pd
from sklearn.metrics import confusion_matrix
import seaborn as sns

pred=model.predict_generator(test_data_gen)
classes=[_ for _ in range(102)]
cm=tf.math.confusion_matrix(true_classes, predicted_classes,num_classes=102)

con_mat_norm = np.around(tf.cast(cm,'float32') / np.sum(cm,axis=1)[:, np.newaxis], decimals=2)
 
con_mat_df = pd.DataFrame(con_mat_norm,
                     index = classes, 
                     columns = classes)

import seaborn as sns
figure = plt.figure(figsize=(17, 17))
sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)
plt.tight_layout()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

plt.savefig('cm.png')

model.save('Trained1.h5')

model.evaluate_generator(test_data_gen)



import numpy as np
test_steps_per_epoch = np.math.ceil(test_data_gen.samples / test_data_gen.batch_size)

predictions = model.predict_generator(test_data_gen, steps=test_steps_per_epoch)
# Get most likely class
predicted_classes = np.argmax(predictions, axis=1)

#predicted_classes.sort()
predicted_classes

true_classes = test_data_gen.classes
class_labels = list(test_data_gen.class_indices.keys())

from sklearn import metrics

report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)

print(confusion_matrix(test_data_gen.classes, predicted_classes))
print('Classification Report')
target_names = class_labels
print(classification_report(test_data_gen.classes, predicted_classes, target_names=target_names))

report = metrics.classification_report(test_data_gen.classes, predicted_classes, target_names=class_labels)
print(report)

from sklearn.datasets import make_circles
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score
from sklearn.metrics import confusion_matrix
from keras.models import Sequential
from keras.layers import Dense

yhat_probs = model.predict(test_data_gen, verbose=0)
# predict crisp classes for test set
yhat_classes = model.predict_classes(test_data_gen, verbose=0)
# reduce to 1d array
yhat_probs = yhat_probs[:, 0]
yhat_classes = yhat_classes[:, 0]

 
# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(test_data_gen.classes, class_labels)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(test_data_gen.classes, class_labels)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(test_data_gen.classes, class_labels)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(test_data_gen.classes, class_labels)
print('F1 score: %f' % f1)
 
# kappa
kappa = cohen_kappa_score(test_data_gen.classes, class_labels)
print('Cohens kappa: %f' % kappa)
# ROC AUC
auc = roc_auc_score(test_data_gen.classes, class_labels)
print('ROC AUC: %f' % auc)
# confusion matrix
matrix = confusion_matrix(test_data_gen.classes, class_labels)
print(matrix)

[_ for _ in range(102)]

